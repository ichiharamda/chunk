Chunker: テキストを効率的にチャンク化するツール
概要
このプロジェクトは、長いテキストを意味のある単位（チャンク）に効率的に分割するためのPythonプログラムです。具体的には、トークナイザーを使ってテキストをトークン単位で処理し、指定されたトークン数に基づいてテキストを分割します。RAG（Retrieval-Augmented Generation）や他のNLPプロジェクトで、モデルのトークン数制限を満たすために使用されることが多いです。

特徴
意味を考慮した分割: 文や単語の途中で不自然に分割しないように、意味のある区切り（句読点など）を優先してチャンクを作成します。
トークナイザー対応: GPT系モデル（例: GPT-3.5-turbo）のトークナイザーに基づいて、モデルの最大トークン数に収まるようにテキストを分割します。
並列処理対応: 複数のテキストを並列処理して、より効率的にチャンク化することが可能です。
カスタム可能: 任意のトークナイザーやトークンカウンターを簡単に設定できます。
必要なライブラリ
このプロジェクトを実行するためには、以下のPythonライブラリが必要です。

re: 正規表現を使用してテキストを分割します。
bisect: リスト内での二分探索に使用します。
functools: キャッシュを利用してパフォーマンスを向上させます。
itertools: 累積計算を簡単に行うために使用します。
tiktoken: GPTモデルのトークナイザーをサポートします。
mpire: 並列処理のためのワーカープールを提供します。
tqdm: プログレスバーを表示します。
インストール
bash
コードをコピーする
pip install tiktoken mpire tqdm
使い方
トークナイザーの設定

python
コードをコピーする
from chunker import chunkerify

# GPT-3.5-turbo用のチャンク作成
chunker = chunkerify("gpt-3.5-turbo", chunk_size=1000)
テキストのチャンク化

python
コードをコピーする
long_text = """
In the year 2024, technological advancements have accelerated...
"""

# チャンクに分割
chunks = chunker(long_text)

# 結果の表示
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}:\n{chunk}\n")
複数テキストの並列処理

python
コードをコピーする
texts = ["text1...", "text2...", "text3..."]

# 並列で処理
chunks = chunker(texts, processes=4, progress=True)
パラメータ
chunk_size: 1つのチャンクに含まれるトークン数の上限を指定します。指定しない場合は、使用するモデルの最大トークン数に基づいて自動設定されます。
token_counter: 任意のトークナイザーに基づいたトークンカウントロジックを定義できます。
processes: 並列処理を使用する場合のプロセス数を指定します。
チャンクの仕組み
テキストを意味のある区切り（例: 句読点、空白文字）で分割します。
分割されたテキストをトークンカウンターを使って計測し、指定されたチャンクサイズに収めるように結合します。
必要に応じて再帰的にテキストを小さなチャンクに分割し、トークンサイズの制約を満たします。
